{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Лабораторная работа 3. Линейные методы\n",
    "\n",
    "Результат лабораторной работы − отчет в формате Jupyter notebook'а. Постарайтесь сделать ваш отчет интересным рассказом, последовательно отвечающим на вопросы из заданий. Помимо ответов на вопросы, в отчете так же должен быть код, однако чем меньше кода, тем лучше всем: мне − меньше проверять, вам —  проще найти ошибку или дополнить эксперимент. При проверке оценивается четкость ответов на вопросы, аккуратность отчета и кода.\n",
    "\n",
    "### Оценивание и штрафы\n",
    "\n",
    "* Каждая из задач имеет определенную «стоимость» (указана в скобках около задачи)\n",
    "* Максимально допустимая оценка за работу — 15 баллов\n",
    "* При сдаче до soft deadline можно получить максимум 100% баллов\n",
    "* При просрочке не более, чем на 5 минут — максимум 90% баллов\n",
    "* При просрочке не более, чем на 1 час — максимум 80% баллов\n",
    "* При сдаче до hard deadline — максиму 70% баллов\n",
    "* Сдавать задание после hard deadline нельзя\n",
    "* «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов и понижают карму\n",
    "* Если вы нашли решение какого-то из заданий в открытом источнике, необходимо прислать ссылку на этот источник (скорее всего вы будете не единственным, кто это нашел, поэтому чтобы исключить подозрение в плагиате, необходима ссылка на источник)\n",
    "* Не оцениваются задания с удалёнными формулировкам\n",
    "* Не оценивается лабораторная работа целиком, если она была выложена в открытый источник"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='log_reg'></a>\n",
    "## Логистическая регрессия\n",
    "\n",
    "В этом пункте мы будем рассматривать бинарную классификацию, где метки классов лежат во множестве $\\{-1, 1\\}$. \n",
    "\n",
    "Задачу обучения регуляризованной логистической регрессии можно записать следующим образом:\n",
    "\n",
    "$$ \\dfrac{1}{N}\\sum_{i=1}^N \\log(1 + \\exp(-y_i(\\langle w, x_i \\rangle + b))) + \\dfrac{C}{2}\\lVert w \\rVert^2  \\to \\min_w$$\n",
    "\n",
    "Обучение в данном случае сводится к нахождению вектора весов модели $w$ и константы $b$, которое производится с помощью метода градиентного спуска (Gradient Descent, GD). \n",
    "\n",
    "Градиентный шаг будет заключаться в обновлении вектора весов по следующей формуле:\n",
    "\n",
    "$$w_{new} := w + \\dfrac{\\eta}{N}\\sum_{i=1}^N y_ix_i \\Big(1 - \\dfrac{1}{1 + exp(-y_i(\\langle w, x_i \\rangle + b))}\\Big) - \\eta Cw$$\n",
    "\n",
    "$$b_{new} := b + \\dfrac{\\eta}{N}\\sum_{i=1}^N y_i \\Big(1 - \\dfrac{1}{1 + exp(-y_i(\\langle w, x_i \\rangle + b))}\\Big)$$\n",
    "\n",
    "$$(w, b) := (w_{new}, b_{new})$$\n",
    "\n",
    "где $\\eta > 0$ — размер шага (learning rate).\n",
    "\n",
    "В общем случае метод градиентного спуска имеет следующие недостатки:\n",
    "- попадание в локальные минимумы\n",
    "- неочевидность критерия останова\n",
    "- выбор размера шага\n",
    "- начальная инициализация весов\n",
    "\n",
    "В этой части лабораторной работы мы предложим вам реализовать метод градиентного спуска, а также рассмотрим некоторые его модификации.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "- Загрузите обучающий набор данных (X_train.csv, y_train.csv) из [набора данных конкурса 2](https://www.kaggle.com/c/competition-2-yandex-shad-spring-2020).\n",
    "- Для простоты в этом задании ограничимся работой с числовыми признаками (Numeric в таблице MetaData.csv).\n",
    "- Заполните пропущенные значения в данных. Для этого могут пригодиться методы из sklearn.impute или pandas.DataFrame.fillna.\n",
    "- Обратите внимание, что метки классов для данной задачи должны быть из множества {-1, 1}, а не {0, 1}.\n",
    "- По желанию используйте любой препроцессинг данных, ваша задача — добиться сходимости и высокого качества полученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "medical_data = [f'V{i}' for i in range(2, 332)]\n",
    "gene_data = [f'V{i}' for i in range(332, 1332)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = np.array(pd.read_csv('data/X_train.csv'))\n",
    "# y_train = np.array(pd.read_csv('data/y_train.csv')['label'])\n",
    "\n",
    "# X_test = np.array(pd.read_csv('data/X_test.csv'))\n",
    "\n",
    "X_train = pd.read_csv('data/X_train.csv')\n",
    "y_train = pd.read_csv('data/y_train.csv')\n",
    "\n",
    "X_test = pd.read_csv('data/X_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4099, 1330)\n",
      "(4099, 1)\n",
      "(1366, 1330)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>...</th>\n",
       "      <th>V1322</th>\n",
       "      <th>V1323</th>\n",
       "      <th>V1324</th>\n",
       "      <th>V1325</th>\n",
       "      <th>V1326</th>\n",
       "      <th>V1327</th>\n",
       "      <th>V1328</th>\n",
       "      <th>V1329</th>\n",
       "      <th>V1330</th>\n",
       "      <th>V1331</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>826.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030303</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>242.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1038.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>971.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       V2   V3  V4   V5   V6   V7   V8        V9       V10  V11  ...  V1322  \\\n",
       "0   826.0  0.0   0  0.0  0.0  0.0  0.0       NaN       NaN  0.0  ...    0.0   \n",
       "1    15.0  0.0   0  0.0  0.0  0.0  0.0  0.030303  0.083333  0.0  ...    0.0   \n",
       "2   242.0  0.0   0  0.0  0.0  0.0  0.0       NaN       NaN  0.0  ...    1.0   \n",
       "3  1038.0  0.0   0  0.0  0.0  0.0  0.0       NaN       NaN  0.0  ...    0.5   \n",
       "4   971.0  0.0   0  0.0  0.0  0.0  0.0       NaN       NaN  0.0  ...    0.0   \n",
       "\n",
       "   V1323  V1324  V1325  V1326  V1327  V1328  V1329  V1330  V1331  \n",
       "0    0.0    0.0    0.0    1.0    1.0    0.0    NaN    0.0    0.5  \n",
       "1    0.0    1.0    0.0    0.0    0.0    0.5    0.0    0.5    0.0  \n",
       "2    1.0    0.0    0.5    0.0    0.0    1.0    0.0    0.0    0.5  \n",
       "3    0.5    0.0    0.0    0.0    0.0    0.0    0.0    0.0    0.5  \n",
       "4    0.0    1.0    0.0    0.5    0.0    0.0    0.0    0.0    0.0  \n",
       "\n",
       "[5 rows x 1330 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>V11</th>\n",
       "      <th>...</th>\n",
       "      <th>V1322</th>\n",
       "      <th>V1323</th>\n",
       "      <th>V1324</th>\n",
       "      <th>V1325</th>\n",
       "      <th>V1326</th>\n",
       "      <th>V1327</th>\n",
       "      <th>V1328</th>\n",
       "      <th>V1329</th>\n",
       "      <th>V1330</th>\n",
       "      <th>V1331</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>685.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1150.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>91.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.010101</td>\n",
       "      <td>0.097222</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>332.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>714.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 1330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       V2   V3  V4   V5   V6   V7   V8        V9       V10  V11  ...  V1322  \\\n",
       "0   685.0  0.0   0  0.0  0.0  0.0  0.0  0.010101  0.097222  0.0  ...    0.5   \n",
       "1  1150.0  0.0   0  0.0  0.0  0.0  0.0  0.010101  0.083333  0.0  ...    0.5   \n",
       "2    91.0  0.0   1  0.0  0.0  0.0  1.0  0.010101  0.097222  0.0  ...    0.0   \n",
       "3   332.0  0.0   1  0.0  0.0  0.0  1.0       NaN       NaN  0.0  ...    0.5   \n",
       "4   714.0  0.0   1  0.0  0.0  1.0  1.0       NaN       NaN  NaN  ...    0.5   \n",
       "\n",
       "   V1323  V1324  V1325  V1326  V1327  V1328  V1329  V1330  V1331  \n",
       "0    0.5    0.5    0.0    0.5    0.5    0.5    0.0    0.0    0.0  \n",
       "1    0.5    0.5    0.0    0.5    0.0    0.0    0.0    0.0    0.5  \n",
       "2    0.5    0.5    0.0    0.0    0.0    0.0    0.0    0.0    0.0  \n",
       "3    0.5    0.5    0.0    0.5    0.0    0.0    NaN    0.0    0.0  \n",
       "4    1.0    0.0    0.0    0.0    0.0    0.0    NaN    0.0    0.0  \n",
       "\n",
       "[5 rows x 1330 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label\n",
       "0      0\n",
       "1      0\n",
       "2      0\n",
       "3      0\n",
       "4      0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>varnum</th>\n",
       "      <th>type</th>\n",
       "      <th>Column Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>V2</td>\n",
       "      <td>char</td>\n",
       "      <td>Numeric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>V3</td>\n",
       "      <td>num</td>\n",
       "      <td>Category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>V4</td>\n",
       "      <td>num</td>\n",
       "      <td>Category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>V5</td>\n",
       "      <td>num</td>\n",
       "      <td>Category</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>V6</td>\n",
       "      <td>num</td>\n",
       "      <td>Category</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  varnum  type Column Type\n",
       "0     V2  char     Numeric\n",
       "1     V3   num    Category\n",
       "2     V4   num    Category\n",
       "3     V5   num    Category\n",
       "4     V6   num    Category"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_data = pd.read_csv('data/MetaData.csv')\n",
    "meta_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['V2', 'V29', 'V31', 'V32', 'V34', 'V35', 'V36', 'V37', 'V38',\n",
       "       'V39', 'V105', 'V111', 'V113', 'V114', 'V116', 'V118', 'V144',\n",
       "       'V149', 'V152', 'V157', 'V158', 'V161', 'V162', 'V165', 'V170',\n",
       "       'V175', 'V180', 'V186', 'V188', 'V189', 'V190', 'V191', 'V192',\n",
       "       'V193', 'V195', 'V197', 'V198', 'V199', 'V201', 'V203', 'V204',\n",
       "       'V205', 'V207', 'V208', 'V209', 'V235', 'V236', 'V237', 'V238',\n",
       "       'V239', 'V240', 'V241', 'V242', 'V243', 'V244', 'V245', 'V246',\n",
       "       'V247', 'V248', 'V249', 'V250', 'V251', 'V252', 'V253', 'V254',\n",
       "       'V255', 'V256', 'V257', 'V258', 'V259', 'V260', 'V261', 'V262',\n",
       "       'V263', 'V264', 'V265', 'V266', 'V267', 'V268', 'V269', 'V270',\n",
       "       'V271', 'V272', 'V273', 'V274', 'V275', 'V276', 'V277', 'V278',\n",
       "       'V279', 'V280', 'V281', 'V282', 'V283', 'V284', 'V285', 'V286',\n",
       "       'V287', 'V288', 'V289', 'V290', 'V291', 'V292', 'V293', 'V294',\n",
       "       'V295', 'V296', 'V297', 'V298', 'V299', 'V300', 'V302', 'V303',\n",
       "       'V304', 'V305', 'V306', 'V308', 'V310', 'V311', 'V312', 'V313',\n",
       "       'V314', 'V315', 'V316', 'V317', 'V318', 'V319', 'V320', 'V321',\n",
       "       'V322', 'V323', 'V324', 'V325', 'V326', 'V327', 'V328', 'V331'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric = meta_data[meta_data['Column Type'] == 'Numeric']['varnum'].values\n",
    "numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4099, 137)\n",
      "(1366, 137)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train[numeric]\n",
    "X_test = X_test[numeric]\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(X_train.isna().sum() > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V2</th>\n",
       "      <th>V29</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>V36</th>\n",
       "      <th>V37</th>\n",
       "      <th>V38</th>\n",
       "      <th>V39</th>\n",
       "      <th>...</th>\n",
       "      <th>V320</th>\n",
       "      <th>V321</th>\n",
       "      <th>V322</th>\n",
       "      <th>V323</th>\n",
       "      <th>V324</th>\n",
       "      <th>V325</th>\n",
       "      <th>V326</th>\n",
       "      <th>V327</th>\n",
       "      <th>V328</th>\n",
       "      <th>V331</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4050.000000</td>\n",
       "      <td>4099.000000</td>\n",
       "      <td>4099.000000</td>\n",
       "      <td>4086.000000</td>\n",
       "      <td>4088.000000</td>\n",
       "      <td>4098.000000</td>\n",
       "      <td>4099.000000</td>\n",
       "      <td>4099.000000</td>\n",
       "      <td>4099.000000</td>\n",
       "      <td>4099.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4037.000000</td>\n",
       "      <td>4037.000000</td>\n",
       "      <td>4037.000000</td>\n",
       "      <td>4037.000000</td>\n",
       "      <td>4080.000000</td>\n",
       "      <td>4030.000000</td>\n",
       "      <td>4030.000000</td>\n",
       "      <td>4030.000000</td>\n",
       "      <td>4030.000000</td>\n",
       "      <td>4099.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>434.000988</td>\n",
       "      <td>0.418759</td>\n",
       "      <td>0.377803</td>\n",
       "      <td>0.068955</td>\n",
       "      <td>0.993885</td>\n",
       "      <td>0.887900</td>\n",
       "      <td>0.410042</td>\n",
       "      <td>0.357092</td>\n",
       "      <td>0.485781</td>\n",
       "      <td>0.322299</td>\n",
       "      <td>...</td>\n",
       "      <td>0.384444</td>\n",
       "      <td>0.610271</td>\n",
       "      <td>0.312917</td>\n",
       "      <td>0.204802</td>\n",
       "      <td>0.231904</td>\n",
       "      <td>0.622429</td>\n",
       "      <td>0.477875</td>\n",
       "      <td>0.522669</td>\n",
       "      <td>0.138306</td>\n",
       "      <td>0.520322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>360.832558</td>\n",
       "      <td>0.104766</td>\n",
       "      <td>0.136818</td>\n",
       "      <td>0.228712</td>\n",
       "      <td>4.069883</td>\n",
       "      <td>0.109836</td>\n",
       "      <td>0.097301</td>\n",
       "      <td>0.138053</td>\n",
       "      <td>0.126246</td>\n",
       "      <td>0.121573</td>\n",
       "      <td>...</td>\n",
       "      <td>0.149358</td>\n",
       "      <td>0.180878</td>\n",
       "      <td>0.140969</td>\n",
       "      <td>0.150743</td>\n",
       "      <td>0.349645</td>\n",
       "      <td>0.060175</td>\n",
       "      <td>0.075524</td>\n",
       "      <td>0.059778</td>\n",
       "      <td>0.037003</td>\n",
       "      <td>0.233743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.016678</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007334</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000176</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>128.000000</td>\n",
       "      <td>0.346535</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.345238</td>\n",
       "      <td>0.260598</td>\n",
       "      <td>0.387399</td>\n",
       "      <td>0.236248</td>\n",
       "      <td>...</td>\n",
       "      <td>0.277590</td>\n",
       "      <td>0.518072</td>\n",
       "      <td>0.210218</td>\n",
       "      <td>0.076955</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.593165</td>\n",
       "      <td>0.437247</td>\n",
       "      <td>0.492253</td>\n",
       "      <td>0.121195</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>331.000000</td>\n",
       "      <td>0.415842</td>\n",
       "      <td>0.373626</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.343989</td>\n",
       "      <td>0.486595</td>\n",
       "      <td>0.304960</td>\n",
       "      <td>...</td>\n",
       "      <td>0.367762</td>\n",
       "      <td>0.662651</td>\n",
       "      <td>0.307895</td>\n",
       "      <td>0.185221</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620144</td>\n",
       "      <td>0.471913</td>\n",
       "      <td>0.515495</td>\n",
       "      <td>0.133203</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>690.000000</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.461538</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.961538</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.434329</td>\n",
       "      <td>0.577748</td>\n",
       "      <td>0.386894</td>\n",
       "      <td>...</td>\n",
       "      <td>0.482393</td>\n",
       "      <td>0.746988</td>\n",
       "      <td>0.406215</td>\n",
       "      <td>0.297662</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.652518</td>\n",
       "      <td>0.514676</td>\n",
       "      <td>0.544100</td>\n",
       "      <td>0.149679</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1361.000000</td>\n",
       "      <td>0.851485</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.839286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.922252</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.877818</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.945683</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.966627</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                V2          V29          V31          V32          V34  \\\n",
       "count  4050.000000  4099.000000  4099.000000  4086.000000  4088.000000   \n",
       "mean    434.000988     0.418759     0.377803     0.068955     0.993885   \n",
       "std     360.832558     0.104766     0.136818     0.228712     4.069883   \n",
       "min       0.000000     0.079208     0.000000     0.000000     0.000000   \n",
       "25%     128.000000     0.346535     0.274725     0.000000     0.000000   \n",
       "50%     331.000000     0.415842     0.373626     0.000000     0.000000   \n",
       "75%     690.000000     0.485149     0.461538     0.000000     0.000000   \n",
       "max    1361.000000     0.851485     0.934066     1.000000    49.000000   \n",
       "\n",
       "               V35          V36          V37          V38          V39  ...  \\\n",
       "count  4098.000000  4099.000000  4099.000000  4099.000000  4099.000000  ...   \n",
       "mean      0.887900     0.410042     0.357092     0.485781     0.322299  ...   \n",
       "std       0.109836     0.097301     0.138053     0.126246     0.121573  ...   \n",
       "min       0.000000     0.119048     0.016678     0.000000     0.007334  ...   \n",
       "25%       0.846154     0.345238     0.260598     0.387399     0.236248  ...   \n",
       "50%       0.923077     0.404762     0.343989     0.486595     0.304960  ...   \n",
       "75%       0.961538     0.476190     0.434329     0.577748     0.386894  ...   \n",
       "max       1.000000     0.839286     1.000000     0.922252     1.000000  ...   \n",
       "\n",
       "              V320         V321         V322         V323         V324  \\\n",
       "count  4037.000000  4037.000000  4037.000000  4037.000000  4080.000000   \n",
       "mean      0.384444     0.610271     0.312917     0.204802     0.231904   \n",
       "std       0.149358     0.180878     0.140969     0.150743     0.349645   \n",
       "min       0.000000     0.000000     0.000176     0.000000     0.000000   \n",
       "25%       0.277590     0.518072     0.210218     0.076955     0.000000   \n",
       "50%       0.367762     0.662651     0.307895     0.185221     0.000000   \n",
       "75%       0.482393     0.746988     0.406215     0.297662     0.500000   \n",
       "max       1.000000     1.000000     1.000000     0.877818     1.000000   \n",
       "\n",
       "              V325         V326         V327         V328         V331  \n",
       "count  4030.000000  4030.000000  4030.000000  4030.000000  4099.000000  \n",
       "mean      0.622429     0.477875     0.522669     0.138306     0.520322  \n",
       "std       0.060175     0.075524     0.059778     0.037003     0.233743  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.593165     0.437247     0.492253     0.121195     0.300000  \n",
       "50%       0.620144     0.471913     0.515495     0.133203     0.500000  \n",
       "75%       0.652518     0.514676     0.544100     0.149679     0.700000  \n",
       "max       0.945683     1.000000     0.966627     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 137 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X_train)\n",
    "\n",
    "X_train = imp.transform(X_train)\n",
    "X_test = imp.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3756\n",
      "0.9163210539155892\n"
     ]
    }
   ],
   "source": [
    "print(sum(y_train['label'] == 0))\n",
    "print(sum(y_train['label'] == 0)/len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3756\n"
     ]
    }
   ],
   "source": [
    "y_train[y_train['label'] == 0] = -1\n",
    "print(sum(y_train['label'] == -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4099, 137)\n",
      "(4099, 1)\n",
      "(1366, 137)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save preprocessed data\n",
    "pd.DataFrame(X_train).to_csv('x_train.csv', index=None)\n",
    "pd.DataFrame(X_test).to_csv('x_test.csv', index=None)\n",
    "pd.DataFrame(y_train).to_csv('y_train.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(4 балла)** Реализуйте градиентный спуск и протестируйте его для случая логистической регрессии на данных конкурса. Для сравнения качества разных подходов используйте значение оптимизируемого функционала.\n",
    "\n",
    "В качестве критерия останова мы предлагаем использовать следующие условия:\n",
    " - евклидова норма разности текущего и нового векторов весов стала меньше, чем 1e-4\n",
    " - ограничение на число итераций (например, 10000)\n",
    " \n",
    "Для начальной инициализации весов нужно сравнить следующие подходы:\n",
    " - нулевая начальная инициализация\n",
    " - случайная\n",
    " \n",
    "Выполните следующие пункты и прокомментируйте полученные результаты:\n",
    "- Рассмотрите как влияет размер шага на сходимость (попробуйте не менее 5-ти различных значений).\n",
    "- Рассмотрите регуляризованную модель (не менее 5-ти различных коэффициентов регуляризации), которая описана выше, а также модель без регуляризатора. Сравните, влияет ли наличие регуляризации на скорость сходимости и качество (под качеством во всех случаях подразумевается значение исходного, нерегуляризованного функционала).\n",
    "- Исследуйте качество оптимизируемого функционала в зависимости от номера итерации (при правильной реализации и подходящем размере шага он должен убывать).\n",
    "- Влияет ли выбор начальной инициализации весов на скорость и качество?\n",
    "\n",
    "В каждом пункте требуется построить необходимые графики скорости/качества и дать исчерпывающие выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GDClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, is_stochastic=False, learning_rate=0.01, n_iter=10000, C=0.01, epsilon=1e-4,\n",
    "                 random_init=False, random_state=42):\n",
    "        self.is_stochastic = is_stochastic\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iter = n_iter\n",
    "        self.C = C\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "        self.random_init = random_init\n",
    "        self.random_state = random_state\n",
    "        \n",
    "        self.weights = None\n",
    "        self.coeffs_ = None\n",
    "        self.b = None\n",
    "\n",
    "    @staticmethod\n",
    "    def sigmoid(x):\n",
    "        return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "    def compute_gradient(self, x, y, w):\n",
    "        sigm = (1 - self.sigmoid(y * x.dot(w)))\n",
    "        grad = np.dot(x.T, y*sigm)\n",
    "        return grad if len(x.shape) == 1 else grad / len(x)\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        x, y = np.array(X_train), np.array(y_train)\n",
    "        x = np.c_[X_train, np.ones(len(y_train))]\n",
    "        if y.ndim == 2:\n",
    "            y = y.reshape((len(y), ))\n",
    "\n",
    "        n_obs = x.shape[0]\n",
    "        n_features = x.shape[1]\n",
    "\n",
    "        self.weights = np.zeros((2, n_features))\n",
    "\n",
    "        if self.random_init:\n",
    "            self.weights[0] = np.random.normal(size=(1, n_features))\n",
    "\n",
    "        for k in range(self.n_iter - 1):\n",
    "            if self.is_stochastic:\n",
    "                ind = np.random.randint(0, n_obs - 1)\n",
    "                x, y = X_train[ind, :], y_train[ind]\n",
    "\n",
    "            w = self.weights[0]\n",
    "            self.weights[0] = self.weights[1]\n",
    "\n",
    "            grad = self.compute_gradient(x, y, w) - w * self.C\n",
    "\n",
    "            u = self.learning_rate * grad\n",
    "            self.weights[1] = self.weights[0] + u\n",
    "            self.weights[1][-1] += self.learning_rate * self.C * w[-1]\n",
    "\n",
    "            if np.linalg.norm(self.weights[0] - self.weights[1]) < self.epsilon:\n",
    "                print(f'Number of iterations: {k}.')\n",
    "                break\n",
    "\n",
    "        self.coeffs_ = self.weights[1][:n_features-1]\n",
    "        self.b = self.weights[1][-1]\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(X.dot(self.coeffs_) + self.b)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of iterations: 4540.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GDClassifier(C=0.01, epsilon=0.0001, is_stochastic=False, learning_rate=0.01,\n",
       "             n_iter=10000, random_init=False, random_state=42)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = GDClassifier()\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score: 0.011710173212978776\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f'Train score: {accuracy_score(y_train, clf.predict(X_train))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод градиентного спуска может быть весьма трудозатратен в случае большого размера обучающей выборки. Поэтому обычно используют метод стохастического градиентного спуска, где на каждой итерации выбирается случайный объект из обучающей выборки и обновление весов происходит сразу по этому объекту. \n",
    "\n",
    "**(1.5 балла)** Реализуйте метод стохастического градиентного спуска (sgd). В этом случае вы можете выбрать наиболее удачный функционал, исходя из предыдущего пункта (с регуляризацией, без), а также схему начальной инициализации весов. Реализуйте метод в виде класса, удовлетворяющего интерфейсу scikit-learn ([тут есть пример](http://danielhnyk.cz/creating-your-own-estimator-scikit-learn/)).\n",
    "\n",
    "Сравните рассмотренные методы (градиентный спуск и sgd) между собой с точки зрения скорости сходимости и качества.\n",
    "\n",
    "- Посмотрите как влияет размер шага на сходимость (попробуйте 4-5 различных значений)\n",
    "- Исследуйте качество оптимизируемого функционала в зависимости от номера итерации\n",
    "\n",
    "Выберите лучший размер шага и сравните качество и скорость сходимости реализованного метода и [его аналога из scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) (не забудьте выставить значения важных параметров tol и learning_rate для сравнения методов в приближённо равных условиях). Насколько получилось лучше/хуже, в чём могут быть причины?\n",
    "\n",
    "В каждом пункте сделайте исчерпывающие выводы, подкреплённые графиками."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Между обновлением вектора весов по всей выборке и на одном объекте есть промежуточный подход — выбирать некоторое случайное подмножество объектов и обновлять веса по нему. Такой подход называется mini-batch. Мы не будем реализовывать этот подход в данной работе, однако иногда его бывает осмысленно использовать на практике. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Один из недостатков sgd состоит в том, что он может не доходить до локального оптимального решения, а осциллировать в окрестности. \n",
    "\n",
    "![](http://sebastianruder.com/content/images/2015/12/without_momentum.gif)\n",
    "\n",
    "Для решения этой проблемы существуют методы, позволяющие устранить этот недостаток, а также ускорить сходимость. Рассмотрим некоторые из них.\n",
    "\n",
    "![](http://nghenglim.github.io/images/2015061300.png)\n",
    "\n",
    "### Momentum\n",
    "\n",
    "Этот метод позволяет направить sgd в нужной размерности и уменьшить осцилляцию. \n",
    "\n",
    "В общем случае он будет выглядеть следующим образом: \n",
    "\n",
    "$$ v_t = \\gamma v_{t - 1} + \\eta \\nabla_{\\theta}{J(\\theta)}$$\n",
    "$$ \\theta = \\theta - v_t$$\n",
    "\n",
    "где\n",
    "\n",
    " - $\\theta$ — вектор параметров (в нашем случае — $w$)\n",
    " - $J$ — оптимизируемый функционал\n",
    " - $\\gamma$ — momentum term (обычно выбирается 0.9)\n",
    " \n",
    "### Adagrad \n",
    "\n",
    "Одной из сложностей является выбор размера шага (*learning rate*). Основное отличие данного метода от SGD состоит в том что размер шага определяется для каждого параметра индивидуально. Этот метод хорошо работает с разреженным данным большого объема. \n",
    "\n",
    "Обозначим градиент по параметру $\\theta_i$ на итерации $t$ как $g_{t,i} = \\nabla_{\\theta}J(\\theta_i)$. \n",
    "\n",
    "В случае sgd обновление параметра $\\theta_i$ будет выглядеть следующим образом:\n",
    "\n",
    "$$ \\theta_{t+1, i} = \\theta_{t, i} - \\eta \\cdot g_{t,i}$$\n",
    "\n",
    "А в случае Adagrad общий шаг $\\eta$ нормируется на посчитанные ранее градиенты для данного параметра:\n",
    "\n",
    "$$ \\theta_{t+1, i} = \\theta_{t, i} - \\dfrac{\\eta}{\\sqrt{G_{t,ii} + \\varepsilon}} \\cdot g_{t,i}$$\n",
    "\n",
    "где $G_t$ — диагональная матрица, где каждый диагональный элемент $i,i$ — сумма квадратов градиентов для $\\theta_{i}$ до $t$-ой итерации. $\\varepsilon$ — гиперпараметр, позволяющий избежать деления на 0 (обычно выбирается около *1e-8*).\n",
    "\n",
    "Так как матрица $G_t$ диагональна, в векторном виде это будет выглядеть следующим образом (здесь $\\odot$ — матричное умножение):\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{G_t + \\varepsilon}} \\odot g_t $$\n",
    "\n",
    "### Adadelta\n",
    "\n",
    "Adadelta, в отличии от Adagrad, рассматривает не все предыдущие значения градиентов, а только последние $k$. Кроме того, сумма градиентов определяется как уменьшающееся среднеее всех предыдущих квадратов градиентов. Текущее среднее $E[g^2]_t$ на итерации $t$ будет вглядеть следующим образом:\n",
    "\n",
    "$$ E[g^2]_t = \\gamma E[g^2]_{t-1} + (1-\\gamma)g_t^2 $$\n",
    "\n",
    "здесь $\\gamma$ аналогична гиперпараметру из метода Momentum.\n",
    "\n",
    "Тогда обновление весов можно записать следующим образом:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Перепишем это немного по-другому:\n",
    "\n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\Delta \\theta_t$$ \n",
    "$$\\Delta \\theta_t = - \\dfrac{\\eta}{\\sqrt{E[g^2]_t + \\varepsilon}} g_t $$ \n",
    "\n",
    "Аналогично среднему для градиентов определим среднее для параметров $\\theta$:\n",
    "\n",
    "$$ E[\\Delta \\theta^2]_t = \\gamma E[\\Delta \\theta^2]_{t-1} + (1-\\gamma)\\Delta \\theta^2 $$\n",
    "\n",
    "Введем обозначение $RMS[p]_t = \\sqrt{E[p]_t + \\varepsilon}$\n",
    "\n",
    "Тогда Adadelta выглядит следующим образом:\n",
    "\n",
    "$$\\Delta \\theta_t = - \\dfrac{RMS[\\Delta \\theta^2]}{RMS[ga^2]} g_t $$ \n",
    "$$ \\theta_{t+1} = \\theta_{t} + \\Delta \\theta_t$$ \n",
    "\n",
    "\n",
    "Более подробно об этих и других способах оптимизации можно прочитать:\n",
    " - [здесь](http://sebastianruder.com/optimizing-gradient-descent/index.html#gradientdescentoptimizationalgorithms) очень хорошее описание различных способов оптимизации, в этом задании мы опираемся на терминологию из данной статьи\n",
    " - статья про [momentum](https://pdfs.semanticscholar.org/97da/c94ffd7a7ac09a4218848300cc7e98569d77.pdf)\n",
    " - оригинальная статья про [adagrad](http://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)\n",
    " - оригинальная статья про [adadelta](http://arxiv.org/pdf/1212.5701v1.pdf)\n",
    " - википедия про [momentum](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Momentum) и [adagrad](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#AdaGrad)\n",
    " - [визуализация](http://imgur.com/a/Hqolp) разных способов оптимизации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Реализуйте метод оптимизации *Momentum* **(1 балл)** и один из *Adagrad*/*Adadelta* **(1.5 балла)**.\n",
    "- Сравните оба метода с классическим sgd с точки зрения скорости сходимости.\n",
    "- Посмотрите как значение гиперпараметра $\\gamma$ влияет на скорость сходимости и качество в методе *Momentum*.\n",
    "\n",
    "В заданиях выше требуется построить графики и описать полученные результаты.\n",
    "\n",
    "Дало ли преимущество использование адаптивного шага в методе *Adagrad*/*Adadelta*?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В результате выполнения заданий выше вы получите базовое решение для [конкурса](https://www.kaggle.com/c/competition-2-yandex-shad-spring-2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Многоклассовая классификация\n",
    "\n",
    "Модель логистической регрессии можно обобщить для случая многоклассовой классификации. Метка класса теперь лежит во множестве $\\{1, 2, ..., K\\}$. Параметры модели $w$ в этом случае являются матрицей размерности $K \\times M$, где $M$ − количество признаков. Обучение модели логистической регрессии в многоклассовом случае будет выглядеть следующим образом:\n",
    "\n",
    "$$ -\\dfrac{1}{N}\\sum_{i=1}^N \\sum_{k=1}^K [y_i = k] \\log\\Big(\\frac{\\exp(\\langle w_k, x_i \\rangle)}{\\sum_{s=1}^K \\exp(\\langle w_s, x_i \\rangle))}\\Big) + \\dfrac{C}{2}\\lVert w \\rVert^2  \\to \\min_w$$\n",
    "\n",
    "Здесь $w_k$ обозначает $k$-ую строку матрицы $w$.\n",
    "\n",
    "Обучать эту модель также можно с помощью градиентного спуска.\n",
    "\n",
    "Кроме того существует другой, более универсальный способ решать задачу многоклассовой классификации. Для этого нужно обучить несколько бинарных моделей классификации, после чего на основании предсказаний по этим моделям вынести окончательный вердикт о принадлежности объекта одному из $K$ классов. Существует две популярные стратегии использования бинарных классификаторов для задачи многоклассовой классификации:\n",
    " - OvR (One-vs-Rest, One-vs-All) − стратегия, при которой каждый из $K$ классификаторов обучается отделять объекты одного класса от объектов всех остальных классов. В качестве предсказания используется тот класс, классификатор которого предсказал наибольшую вероятность среди всех.\n",
    " - OvO (One-vs-One) − стратегия, при которой каждый из $\\frac{K(K-1)}{2}$ классификаторов учится разделять объекты пары классов, игнорируя объекты всех остальных классов. На этапе предсказания класс обычно выбирается путем голосования по вердиктам каждого из классификаторов.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1.5 балла)** \n",
    "Сгенерируйте несколько выборок точек с 2 признаками и 3 классами (по 100 объектов каждого класса) на которых будете проводить эксперименты. Для этого можно воспользоваться функцией [make_blobs](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) из пакета sklearn.\n",
    "\n",
    "Обучите [логистическую регрессию](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) из пакета sklearn тремя различными способами:\n",
    " - в режиме multinomial, оптимизирующем многоклассовую функцию потерь\n",
    " - в режиме OvR\n",
    " - в режиме OvO\n",
    " \n",
    "Первые два способа реализованы в самом классе LogisticRegression, в то время как для решения задачи третьим методом в sklearn реализован класс [OneVsOneClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html) (класс для OvR схемы [OneVsRestClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html), конечно, также присутствует в пакете).\n",
    " \n",
    " \n",
    " Проделайте следующие шаги для каждой стратегии и прокомментируйте полученные результаты:\n",
    " - Изобразите точки выборки, а также разделяющие прямые (их должно быть по 3 для каждой из стратегий). Проведите эксперимент на всех сгенерированных выборках.\n",
    " - Какие особенности, преимущества и недостатки с точки зрения построения разделяющих плоскостей, качества разделения классов и вычислительной эффективности характерны для каждого метода? Дайте развёрнутый ответ.\n",
    " - Для каждой из стратегий приведите примеры ситуаций, когда стоит выбирать ее для решения задачи многоклассовой классификации. Обоснуйте\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия\n",
    "\n",
    "Метод градиентного спуска позволяет оптимизировать произвольные функции. Например, рассмотрим задачу линейной регрессии, где $y \\in \\mathbb{R}$, а алгоритм будет иметь вид $a(x) = \\langle w, x\\rangle$. В случае метода наименьших квадратов оптимизируемый функционал можно записать следующим образом:\n",
    "\n",
    "$$ \\sum_{i=1}^N (\\langle w, x_i \\rangle - y_i) ^ 2 \\to \\min_w$$\n",
    "\n",
    "Эта задача интересна тем, что для нее можно выписать аналитическое решение. Попробуем сравнить эти подходы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сгенерируйте выборку из 600 точек с двумя признаками для задачи регрессии, воспользовавшись функцией [make_regression](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_regression.html#sklearn.datasets.make_regression). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Выпишите как выглядит точное решение задачи линейной регрессии. Решите задачу регрессии с помощью этого подхода без использования и с использованием регуляризации. Есть ли недостатки у такого подхода к решению задачи?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(0.5 балла)** Примените метод стохастического градиентного спуска реализованный ранее. Сильно ли отличается полученный вектор параметров по сравнению с точным решением? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "К сожалению, линейная регрессия позволяет хорошо восстанавливать *линейные* зависимости, однако в общем случае хуже работает с более сложными данными. Это хорошо можно увидеть на следующем примере.\n",
    "\n",
    "Пусть исходная зависимость имеет вид $y = x \\cdot sin(x)$. Сгенерируем соответствующие точки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = np.linspace(0, 10, 100)\n",
    "y = X * np.sin(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plt.scatter(X_train, y_train)\n",
    "_ = plt.scatter(X_test, y_test, c='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если теперь к полученным данным применить модель линейной регрессии, то получим следующее решение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train[:, np.newaxis], y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_ = plt.scatter(X_train, y_train)\n",
    "_ = plt.scatter(X_test, y_test, c='g')\n",
    "y_plot = lr.predict(X[:, np.newaxis])\n",
    "_ = plt.plot(X, y_plot, c='r')\n",
    "_ = plt.legend(('linear regression', 'train', 'test'), bbox_to_anchor=(1.05, 1), loc=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество модели регрессии можно оценивать с помощью некоторых метрик, например $MSE = \\sum_{i=1}^l(a(x_i) - y_i)^2$ и в данном случае оно равно:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean_squared_error(y_test, lr.predict(X_test[:, np.newaxis])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построенное решение совсем отдаленно напоминает исходную зависимость. Одним из способов улучшить результат является добавление всех попарных произведений признаков, а также степеней: $x_1, \\dots, x_d, x_1^2, x_1x_2, \\dots, x_d^2, \\dots$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "**(0.5 балла)** Воспользуйтесь классом [PolynomialFeatures](http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html#sklearn.preprocessing.PolynomialFeatures) и добавьте к данным зависимости $y = x \\cdot sin(x)$ различные полиномы.\n",
    "\n",
    " - рассмотрите как степень полинома (от 1 до 20) влияет на качество\n",
    " - изобразите на графике предсказание аналогично линейной регрессии\n",
    " - сравните этот подход с функцией [polyfit](http://docs.scipy.org/doc/numpy-1.10.0/reference/generated/numpy.polyfit.html)\n",
    "\n",
    "В чем могут быть недостатки такого подхода?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейный SVM\n",
    "\n",
    "Вернемся к задаче бинарной классификации. Будем обозначать обучающую выборку $\\{(x_n, y_n)\\}_{n=1}^N$, где $N$ — количество объектов, $\\boldsymbol x_n \\in \\mathbb{R}^d$ — числовой вектор признакового описания объекта, $y_n \\in \\{+1, -1\\}$ — класс объекта.\n",
    "\n",
    "SVM обучает модель разделяющей гиперплоскости:\n",
    "$$f(\\boldsymbol x) = \\boldsymbol w^T \\boldsymbol x + b$$\n",
    "Параметры модели — вектор весов $\\boldsymbol w \\in \\mathbb{R}^d$ и сдвиг $b \\in \\mathbb{R}$.\n",
    "\n",
    "Обучение модели происходит путем решения оптимизационной задачи:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\frac{1}{2} \\| \\boldsymbol w \\|^2 + C \\sum_{n=1}^N \\xi_n \\to \\min_{\\boldsymbol w, \\boldsymbol \\xi, b} \\\\\n",
    "    \\text{s.t.: } \\quad y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1 - \\xi_n, \\quad \\xi_n \\geq 0, \\quad \\forall n=1,\\dots,N\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Ограничения вида $y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1$ требуют, чтобы объекты правильно классифицировались разделяющей гиперплоскостью. Поскольку линейная разделимость выборки не гарантируется на практике, вводят переменные $\\xi_n$ (slack variables), которые ослабляют ограничения правильной классификации. В оптимизируемом функционале слагаемое $\\| \\boldsymbol w \\|^2$ штрафует малую ширину разделяющей гиперплоскости, сумма $\\sum_n \\xi_n$ штрафует ослабление ограничений. \n",
    "\n",
    "После нахождения решения оптимизационной задачи $(\\boldsymbol w_{\\star}, \\boldsymbol \\xi_{\\star}, b_{\\star})$, часть ограничений становятся _активными_, т.е. переходят в \"крайнее положение\" — точное равенство:\n",
    "$$\\quad y_n (\\boldsymbol w_{\\star}^T \\boldsymbol x_n + b_{\\star}) = 1 - \\xi_{\\star,n}$$\n",
    "Объекты, соответствующие активным ограничениям, называются _опорными_.\n",
    "\n",
    "Гиперпараметр $C$ задает баланс между шириной разделяющей полосы и ошибками, допускаемыми классификатором. Обратите внимание, что $C$ фиксируется до обучения и не оптимизируется вместе с параметрами модели. Этот гиперпараметр отвечает за обобщающую способность разделяющей гиперплоскости, высокая обобщающая способность (соответствующая большому значению $C$) может привести к переобучению, если линейная модель хорошо описывает обучающие примеры. При подборе оптимального гиперпараметра $C$ необходимо оценивать качество на отложенной выборке или кросс-валидации. Как правило, для конкретной задачи заранее неизвестно, какой порядок имеет оптимальное значение гиперпараметра $C$, поэтому перебирать значения лучше по логарифмической сетке, например: $10^{-3}, 10^{-2}, \\dots, 10^{5}$.\n",
    "\n",
    "Особенность этого метода в том, что он имеет решение, которое может быть найдено используя квадратичное программирование. В этом задании мы не будем сводить данную задачу к задаче квадратичного программирования, а воспользуемся готовой реализацией из библиотеки sklearn.\n",
    "\n",
    "### Особенности реализации\n",
    "\n",
    "Обратите внимание, что в библиотеке sklearn можно найти 2 реализации линейного SVM: [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) и [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) с ядровой функцией *linear*. Эти реализации имеют различие в библиотеках, на которых основаны: в первом случае используется библиотека *liblinear*, во втором — *libsvm*. Каждая из библиотек имеет свои плюсы, поэтому перед применением стоит определиться какая из реализаций подходит больше. Обратите внимание, что это различие есть только для линейного SVM.\n",
    "\n",
    "В данном задании рекомендуем использовать класс [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) с параметром *kernel='linear'*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "Сгенерируйте три случайные двумерные выборки для бинарной классификации (хотя бы по 400 точек в каждой):\n",
    "- с линейно-разделимыми классами\n",
    "- с хорошо разделимыми классами, но не линейно\n",
    "- с плохо разделимыми классами по имеющимся признакам\n",
    "    \n",
    "Для генерации случайной выборки можно воспользоваться функциями, которые находятся в пакете [sklearn.datasets](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.datasets). Для того чтобы выборки не менялись при перезапуске ноутбука, фиксируйте параметр *random_state*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(1 балл)** Протестируйте линейный SVM  на сгенерированных выборках. Покажите на плоскости разделяющую прямую и линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Проделайте это для разных значений параметра $C$ (не менее 4-х).\n",
    "Как зависит число опорных векторов от параметра $C$ для различных выборок?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Явное преобразование признаков\n",
    "\n",
    "Как и в случае с линейной регрессией, когда оптимальная разделяющая гиперплоскость не является линейной, данная модель является очень грубым решением. Линейная неразделимость объектов может быть исправлена путем перехода в другое признаковое пространство, в котором линейная модель лучше описывает данные и, возможно, существует правильно классифицирующая разделяющая гиперплоскость:\n",
    "\n",
    "$$\\boldsymbol x \\in \\mathbb{R}^d \\mapsto \\phi(\\boldsymbol x) \\in \\mathbb{R}^t$$\n",
    "\n",
    "Так, например, аналогичное добавление всех попарных произведений признаков: $\\phi(x_1, \\dots, x_d) = (x_1, \\dots, x_d, x_1^2, x_1x_2, \\dots, x_d^2)$ переводит в пространство, в котором линейная гиперплоскость является квадратичной формой в исходном пространстве и в исходном пространстве признаков разделяющая поверхность может быть, скажем, эллипсом.\n",
    "\n",
    "[Видеоролик с демонстрацией](https://youtu.be/9NrALgHFwTo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Двойственный переход и Ядровой SVM\n",
    "\n",
    "![](http://i.imgur.com/bJAzRCt.png)\n",
    "\n",
    "Задачу обучения линейного SVM, рассмотренную в предыдущем пункте принято называть _прямой_ оптимизационной задачей для SVM. Любая задача оптимизации с ограничениями имеет [_двойственную_ задачу Лагранжа](http://goo.gl/OujTPr), в которой оптимизируются _двойственные переменные_ (множители Лагранжа), соответствующие штрафу за нарушение ограничений, максимизируется нижняя оценка функционала прямой задачи. В случае задачи квадратичного программирования, решение двойственной задачи (значение оптимизируемого функционала) совпадает с оптимумом прямой задачи.\n",
    "\n",
    "Подробнее можно почитать в [статье](http://www.machinelearning.ru/wiki/images/2/25/SMAIS11_SVM.pdf).\n",
    "\n",
    "Двойственная задача для SVM имеет вид:\n",
    "$$\n",
    "\\begin{gather}\n",
    "    \\sum_{n} \\alpha_n - \\frac{1}{2}\\sum_{n}\\sum_{n'} \\alpha_{n}\\alpha_{n'} y_{n}y_{n'} x_{n}^Tx_{n'} \\to \\max_{\\alpha} \\\\\n",
    "    \\begin{aligned}\n",
    "        \\text{s.t. } \\quad  \n",
    "        & 0 \\le \\alpha_n \\le C, \\quad \\forall n = 1, \\dots, N \\\\\n",
    "        & \\sum_{n} \\alpha_n y_n = 0\n",
    "    \\end{aligned}\n",
    "\\end{gather}\n",
    "$$\n",
    "\n",
    "Оптимизируется вектор из двойственных переменных $\\alpha_n$, соответствующих объектам обучающей выборки. Объект $x_n$ является опорным, если $\\alpha_n > 0$.\n",
    "\n",
    "Предсказание вычисляется по следующему правилу:\n",
    "$$\\hat{y}(x) = \\text{sign}\\left(\\sum_{n}\\alpha_{n}y_{n}x^Tx_{n} + b\\right).$$\n",
    "\n",
    "Для предсказания необходимо оценить значение $b$. Известно, что для любого опорного объекта, который классифицируется безошибочно верно:\n",
    "$$y_n = \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'} + b,$$\n",
    "значит для любого такого объекта:\n",
    "$$b = y_n - \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'}.$$\n",
    "\n",
    "В случае наличия ошибок классификации обучающей выборки, предлагается усреднять значение $b$ по всем опорным векторам:\n",
    "$$b = \\frac{1}{N_\\text{SV}}\\sum_{n \\in \\text{SV}}\\left(y_n - \\sum_{n'}\\alpha_{n}y_{n}x_{n}^Tx_{n'}\\right).$$\n",
    "Интуиция здесь такова, что суммарные ошибки в положительную сторону примерно равны суммарным ошибкам в отрицательную сторону.\n",
    "\n",
    "Другой вариант — отказаться от параметра $b$ и работать с моделью $f(x) = w^Tx$, добавив к вектору $x$ константный признак.\n",
    "\n",
    "#### Неявное преобразование признаков\n",
    "Отметим, что двойственная задача SVM содержит вектора признаков исключительно в виде скалярного произведения $x^Tx'$. Эта особенность позволяет производить неявное преобразование признакового пространства. Вместо вычисления функции $\\phi(\\boldsymbol x)$, которая может отображать исходные признаки в вектора очень большой размерности, будем вычислять скалярное произведение $k(\\boldsymbol x, \\boldsymbol x') = \\phi(\\boldsymbol x)^T\\phi(\\boldsymbol x')$ называемое _ядром_. \n",
    "\n",
    "\n",
    "В этом задании используйте класс $sklearn.svm.SVC$, меняя тип ядра. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание\n",
    "\n",
    "**(1 балл)** Протестируйте на предыдущих двумерных выборках ядровой SVM. Покажите на плоскости строящиеся разделяющие поверхности, линии уровня, ограничивающие коридор $f(\\boldsymbol x) = \\pm 1$. Выделите опорные вектора точками другой формы или большего размера. Попробуйте следующие ядровые функции:\n",
    "- линейная: $k(x, x') = x^Tx'$\n",
    "- полиномиальная: $k(x, x') = (x^Tx' + 1)^d$ с различными степенями $d = 2,3,\\dots$\n",
    "- Гауссовская-RBF: $k(x, x') = \\exp(-\\sigma\\|x - x'\\|^2)$\n",
    "\n",
    "Ответьте на следующие вопросы:\n",
    " - Как ведет себя SVM с полиномиальным ядром в зависимости от параметров $C$ и степени ядра $d$?\n",
    " - Как ведет себя SVM с RBF-ядром в зависимости от параметров $C$ и $\\sigma$? Поварьируйте параметры $C$ и $\\sigma$ по логарифмической сетке. Какие значения параметров ведут к переобучению, а какие — к слишком грубой модели?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы\n",
    "\n",
    "**(2 балла)** В этой работе вы рассмотрели некоторые линейные модели машинного обучения, а также способы их обучения. Ответьте на следующие вопросы:\n",
    "\n",
    " - Какие есть достоинства у рассмотренных моделей? Поясните свой ответ для каждой модели.\n",
    " - Каким общим недостатком обладают данные модели? Какие есть способы его устранения? В чем может заключаться сложность использования этого подхода?\n",
    " - В чем заключаются различия с точки зрения обучения алгоритмов? Какие есть достоинства и недостатки у рассмотренных методов обучения?\n",
    " - Предположите в каком случае каждый из алгоритмов будет работать лучше: при большом/небольшом количестве данных? Поясните почему. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
